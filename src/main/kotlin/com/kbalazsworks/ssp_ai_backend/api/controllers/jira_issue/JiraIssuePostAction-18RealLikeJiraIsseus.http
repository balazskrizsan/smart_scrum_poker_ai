POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"101\",\"key\":\"DEV-101\",\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/101\",\"fields\":{\"summary\":\"WebSocket connection drops intermittently during story point estimation\",\"description\":\"During active sprint planning sessions, the backend WebSocket server occasionally drops connections without any clear pattern. This seems to happen more frequently when more than 5 users are connected and interacting simultaneously. Logs indicate a sudden termination of the session with no corresponding error or timeout. This leads to clients having to reconnect manually, and in some cases, messages are lost entirely. No reconnection attempts are triggered automatically. The issue has been observed on both local and staging environments, and is not limited to specific browsers or frontend versions. We've verified that the WebSocket configuration on the server side does not include any specific idle timeout or connection limit. CPU and memory usage during the incident are within acceptable levels. A deeper inspection of the message handler lifecycle and threading model may be necessary. We suspect a race condition or resource contention could be the root cause.\",\"issuetype\":{\"name\":\"Bug\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-7-1T8:00:00.000+0000\",\"updated\":\"2024-08-2T12:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-101/watchers\",\"watchCount\":1},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"Have we ruled out frontend heartbeat or ping messages being dropped? Some WebSocket implementations close connections silently if no keepalive is detected for a while, even if the server itself doesn't set an idle timeout.\",\"created\":\"2024-08-1T8:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Good question — yes, we verified that the frontend sends a ping frame every 20 seconds, and packet captures confirm they’re reaching the server. The drops still happen even when those pings are flowing regularly.\",\"created\":\"2024-08-1T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"body\":\"Could this be related to connection limits or socket exhaustion at the OS level? ulimit or ephemeral port exhaustion can cause abrupt terminations without clear logging if too many sockets are opened quickly.\",\"created\":\"2024-08-1T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"I checked that — the ulimit -n is set to 65535 on the staging box and netstat doesn't show anything close to exhaustion. Also, when the drop happens, the number of open WebSocket sessions is well under 50. So probably not a system-level limit.\",\"created\":\"2024-08-1T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Then we should capture a thread dump right after a drop event and inspect whether any message handlers or WebSocket lifecycle methods are blocked or deadlocked. If we’re dealing with a race in the dispatcher or message queue, that would explain the silence in the logs and the inconsistent reproduction.\",\"created\":\"2024-08-1T11:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":5,\"total\":5,\"histories\":[{\"id\":\"40001\",\"created\":\"2024-08-1T8:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40002\",\"created\":\"2024-08-1T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]},{\"id\":\"40003\",\"created\":\"2024-08-1T11:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10002\",\"fromString\":\"Blocked\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40004\",\"created\":\"2024-08-2T15:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40005\",\"created\":\"2024-08-2T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"102\",\"key\":\"DEV-102\",\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/102\",\"fields\":{\"summary\":\"PGVector similarity search returns incorrect results with 3072-dim embeddings\",\"description\":\"The semantic search feature yields irrelevant results when using the 3072-dimensional embeddings stored in the jira_ticket_embeddings table. Upon inspection, the cosine similarity calculation appears to be applied on incorrectly normalized vectors. The issue does not exist with the 1536-dim variant, suggesting that the bug is specific to the extended dimensionality handling. A likely cause is a faulty type coercion or array misalignment when inserting or retrieving data from PostgreSQL. Queries return vectors that are clearly not semantically related, yet score higher than genuinely similar tickets. This undermines user trust in the system and makes internal QA impossible. Vector validation tests are missing, and malformed data could be silently corrupting the search index. This issue may also propagate to sprint-level AI summaries or history-based recommendations. We need to verify the integrity of the embeddings and inspect all transformation steps between model output and database storage.\",\"issuetype\":{\"name\":\"Bug\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-7-5T8:00:00.000+0000\",\"updated\":\"2024-08-5T12:00:00.000+0000\",\"customfield_10016\":13,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-102/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Could this be due to a shape mismatch or partial truncation during insert? Are we sure that all 3072 elements are preserved and in the right order when writing the embeddings to the jira_ticket_embeddings table?\",\"created\":\"2024-08-1T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"We double-checked the insert path — the 3072-element float array is passed from the embedding service as a JSON array, then mapped to pgvector(3072) using JOOQ. However, during deserialization, we noticed that in some cases, the vector is coerced into a smaller float array if the input contains unexpected whitespace or trailing commas. That doesn't happen with the 1536-dim variant, which is strictly validated. We may need to introduce explicit vector shape validation before insert.\",\"created\":\"2024-08-1T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Let’s add a database constraint or application-level assertion that rejects any embedding with a dimension count not equal to 3072. We should also backfill a checksum audit to find corrupted rows and confirm if they’re the ones producing bad results. Once that’s done, we can retrain or re-embed affected records and verify similarity scoring against a trusted test set.\",\"created\":\"2024-08-1T11:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40010\",\"created\":\"2024-08-1T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40011\",\"created\":\"2024-08-2T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10004\",\"toString\":\"Under Review\"}]},{\"id\":\"40014\",\"created\":\"2024-08-2T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"103\",\"key\":\"DEV-103\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/103\",\"fields\":{\"summary\":\"Real-time votes not reflected in story card until full page reload\",\"description\":\"During planning poker sessions, when team members cast their votes, the story card UI does not update in real-time for all connected clients. Even though the backend correctly receives and broadcasts vote updates via WebSocket, the frontend sometimes fails to render the new votes until the user refreshes the page. This seems to be caused by the vote update events not being properly handled by the Redux/WebSocket bridge. In some cases, the payload is received but discarded due to mismatched storyId or unexpected message structure. As a result, participants see outdated vote counts or think their input was not registered. This breaks trust in the planning process and causes duplicate submissions or confusion. The vote component should be listening to the correct event channel and update its local state immediately on vote change. A full regression test is needed to ensure event propagation and state updates work across reconnects and simultaneous sessions.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-6-5T8:00:00.000+0000\",\"updated\":\"2024-08-2T18:00:00.000+0000\",\"customfield_10016\":3,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-103/watchers\",\"watchCount\":3},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Have we tested whether the issue only occurs when users join an ongoing session after voting has already started? Maybe they're missing some initial state or subscriptions required to receive live updates.\",\"created\":\"2024-08-1T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"es, we confirmed that late joiners sometimes fail to bind to the correct WebSocket channel or don't receive the backlog of votes that already happened. We’ll need to implement a state sync mechanism on join that pulls the current voting state from the server and ensures the live connection is correctly subscribed to the story-specific channel.\",\"created\":\"2024-08-1T11:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40010\",\"created\":\"2024-08-1T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40011\",\"created\":\"2024-08-2T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10004\",\"toString\":\"Under Review\"}]},{\"id\":\"40014\",\"created\":\"2024-08-2T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"104\",\"key\":\"DEV-104\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/104\",\"fields\":{\"summary\":\"Implement offline-first support with optimistic UI for planning sessions\",\"description\":\"To improve the resilience and usability of planning sessions in environments with poor or intermittent internet connectivity, we need to implement offline-first behavior across key components of the frontend application. This includes local caching of tickets, story point selections, user sessions, and voting results using browser storage mechanisms like IndexedDB or localStorage. When the client is offline, actions such as selecting story points or switching sprints should queue up locally and be replayed automatically once connectivity is restored. Additionally, an optimistic UI pattern should be introduced so that interactions feel immediate, even before server acknowledgment. This involves designing a layer that can temporarily assume success on user actions and revert or retry intelligently on failure. The implementation will require architectural changes, such as abstracting API and WebSocket calls through an action queue manager and adding network state detection hooks to all interactive components. We must also clearly surface the connection status to the user with appropriate toasts or banners, and provide a recovery strategy if desync is detected between client and server state. Special attention needs to be paid to potential merge conflicts when the same ticket is updated by multiple users offline. The project will require breaking down the work into multiple sub-tasks: connection monitoring, local caching, optimistic rendering, action queue management, and conflict resolution. Proper testing will also involve simulating network disruptions and verifying state reconciliation in complex scenarios. This feature is critical for distributed teams working in low-reliability environments and is expected to span at least two sprints including QA and regression coverage.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"In Progress\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-8-0T8:00:00.000+0000\",\"updated\":\"2024-08-12T18:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-104/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-1T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40031\",\"created\":\"2024-08-11T11:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]},{\"id\":\"40032\",\"created\":\"2024-08-12T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10002\",\"fromString\":\"Blocked\",\"to\":\"10001\",\"toString\":\"In Progress\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"105\",\"key\":\"DEV-105\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/105\",\"fields\":{\"summary\":\"Monitoring alerts not firing due to misconfigured Prometheus alert rules\",\"description\":\"Critical system alerts for high memory usage and API error rates are not triggering as expected, even when the Prometheus dashboard clearly shows threshold breaches. After investigation, it turns out the alerting rules were recently modified and now contain incorrect label selectors that prevent the expressions from matching actual data points. Additionally, the evaluation interval for some rules was inadvertently changed from 1 minute to 15 minutes, reducing alert granularity. These misconfigurations make it impossible to detect and react to real-time production issues, putting service availability at risk. The broken rules were not caught during CI validation since they syntactically passed, but lacked proper test coverage. We need to enforce stricter linting and introduce integration tests for all Prometheus rules before deployment. A manual audit of all current alerting rules should be conducted to verify their correctness. AlertManager also needs to be reviewed to ensure it routes notifications based on updated severity labels. Proper end-to-end testing and a rollback mechanism for rule changes should be added to the deployment pipeline.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-3-0T8:00:00.000+0000\",\"updated\":\"2024-08-4T18:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-105/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":5,\"total\":5,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-1T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40031\",\"created\":\"2024-08-3T11:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]},{\"id\":\"40032\",\"created\":\"2024-08-3T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10002\",\"fromString\":\"Blocked\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-4T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-4T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"106\",\"key\":\"DEV-106\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/106\",\"fields\":{\"summary\":\"Ticket embedding not updated when description changes in Jira\",\"description\":\"Whenever a Jira ticket’s description is updated, the change is not reflected in the jira_ticket_embeddings table. Our backend is supposed to fetch the new text content and re-generate embeddings using OpenAI embeddings API, but this pipeline seems to be broken. No update call is triggered, and there are no logs indicating any webhook or polling logic being hit. We have verified that Jira webhooks are configured properly and fire on description updates. The issue may be on the backend’s webhook handler, which might be discarding or ignoring the event payload. As a result, semantic search across tickets returns outdated content, causing users to miss relevant results. This affects the accuracy of both single-ticket retrieval and sprint-level analysis. A short-term fix could involve manually triggering re-embedding via a script, but a permanent fix is needed in the webhook consumption flow.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-6-10T8:00:00.000+0000\",\"updated\":\"2024-08-4T18:00:00.000+0000\",\"customfield_10016\":3,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-106/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"Are we sure the Jira webhook actually includes the updated description in its payload? Maybe it’s firing, but the relevant field isn’t present or has a different path than expected.\",\"created\":\"2024-08-2T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"Yes, we inspected the raw payload — the issue.fields.description field is present and contains the updated text. So the data is there, the problem seems to be further downstream in our processing.\",\"created\":\"2024-08-2T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"body\":\"Is the webhook handler even receiving the requests? Could something in the infrastructure (e.g. firewall, reverse proxy) be silently dropping or misrouting them?\",\"created\":\"2024-08-2T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"We checked the logs of the service that handles webhooks, and there's no sign of incoming requests on PUT /jira/webhook. That endpoint works for other events like ticket creation, so it’s possible the descriptionUpdated event is being sent to a different route or not mapped correctly in our router.\",\"created\":\"2024-08-2T13:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Let’s add a fallback catch-all route to log any unmatched webhook traffic and confirm we’re not discarding valid events due to misconfigured routes. We should also add temporary verbose logging for all webhook payloads to help with debugging.\",\"created\":\"2024-08-2T14:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Once we confirm the webhook is flowing through correctly, we also need to verify that the embedding update queue is consuming and processing the update jobs. Even if the handler is fixed, outdated queue logic could prevent the actual re-embedding from happening. We might need to invalidate and requeue stale tickets as part of the fix.\",\"created\":\"2024-08-2T15:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":5,\"total\":5,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-2T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40031\",\"created\":\"2024-08-4T11:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]},{\"id\":\"40032\",\"created\":\"2024-08-4T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10002\",\"fromString\":\"Blocked\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-4T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-4T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"107\",\"key\":\"DEV-107\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/107\",\"fields\":{\"summary\":\"Backend allows duplicate sprint names within the same board\",\"description\":\"Our backend currently permits creating multiple sprints with identical names under the same Jira board, even though Jira enforces unique names per board. This leads to ambiguity in analytics queries and user-facing reports. Users have reported confusion when trying to generate burndown charts or fetch sprint-specific embeddings, as it's unclear which sprint instance is being referenced. The bug is caused by the missing unique constraint at the database level and lack of validation in the createSprint() service. Because our logic uses sprint names for lookup in several places, having duplicates breaks assumptions in filtering, aggregating, and displaying sprint data. This could also cause accidental overwrites during re-sync operations. Fixing this requires enforcing uniqueness both in the DB schema and in the application layer, with appropriate error messages returned to the frontend.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-6-22T5:00:00.000+0000\",\"updated\":\"2024-08-5T18:00:00.000+0000\",\"customfield_10016\":2,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-107/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Do we rely on the sprint name as a natural key in any of our internal logic, or is the sprint ID always used as the primary identifier in downstream services?\",\"created\":\"2024-08-4T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Unfortunately, several parts of the analytics pipeline and embedding lookup logic still use sprint name as a key when resolving data within a board context, especially in legacy code. Until we refactor those paths to rely strictly on sprint_id, allowing duplicate names will continue to break aggregation and report generation. So the fix needs to include both DB-level uniqueness and validation in the createSprint() logic to reject duplicates early.\",\"created\":\"2024-08-4T10:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-5T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-5T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-5T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"108\",\"key\":\"DEV-108\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/108\",\"fields\":{\"summary\":\"Sprint analytics endpoint returns 500 when no tickets exist\",\"description\":\"When requesting sprint analytics for a sprint that contains zero tickets, the backend throws a 500 Internal Server Error. This is due to a failure in the statistical aggregation query, which does not handle empty result sets properly. Instead of returning a default structure with zeros or empty arrays, the query attempts to perform operations like average and percentile on null values. Stack traces indicate the error originates from the analytics service layer, specifically when computing ticket velocity and point distribution. The response should gracefully handle such cases and return valid but empty metrics. This issue blocks the ability to generate dashboards for newly created sprints or archived ones that had no activity. It also causes automated CI checks to fail in our reporting pipeline, which expects a valid response schema. A test should be added for the edge case where a sprint has no tickets at all.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-0-1T8:00:00.000+0000\",\"updated\":\"2024-08-8T12:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-108/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":5,\"total\":5,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-5T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-8T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-8T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"109\",\"key\":\"DEV-109\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/109\",\"fields\":{\"summary\":\"Deployment rollback fails due to persistent volume claims not being detached\",\"description\":\"Rolling back failed deployments on our Kubernetes cluster sometimes fails due to PersistentVolumeClaims (PVCs) remaining attached to the terminated pods. This causes subsequent pods to get stuck in a Pending or CrashLoopBackOff state because they cannot acquire the required volume. The issue occurs mostly with workloads using ReadWriteOnce PVCs and StatefulSets during quick redeploys or high-load situations. Our current Helm rollback strategy does not wait for volume detachment before reapplying previous manifests, leading to race conditions. There are no clear cleanup hooks in place, and volume retention policies are inconsistently applied across namespaces. As a result, failed releases block further recovery attempts, requiring manual pod deletion or PVC force-detach. We need to revisit our storage class settings, implement pre-stop hooks, and ensure our rollback automation handles volume lifecycle correctly. Logging and monitoring should be improved to surface these issues earlier, and retention policies should be unified to avoid orphaned claims.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-4-1T8:00:00.000+0000\",\"updated\":\"2024-08-8T18:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-109/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-5T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-8T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-8T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"110\",\"key\":\"DEV-110\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/110\",\"fields\":{\"summary\":\"Infrastructure drift detected between Terraform state and live AWS environment\",\"description\":\"Our weekly drift detection run has flagged multiple inconsistencies between the Terraform state and the actual AWS infrastructure. Several S3 buckets, IAM policies, and RDS instance parameters differ from what’s declared in code. This drift likely comes from manual changes introduced through the AWS Console or ad-hoc CLI scripts that bypassed infrastructure as code. The most critical discrepancy involves an IAM policy that is broader in production than declared, posing a potential security risk. The affected resources are spread across two environments (staging and prod) and involve multiple contributors, making root cause analysis difficult. We need to audit recent manual changes and investigate why CI/CD pipelines did not prevent them. In the long term, we should enforce stricter policies around drift via automated rollback or alerts. Terraform state should be treated as the single source of truth, and manual overrides must be reviewed and reverted if unauthorized. Additional controls such as policy-as-code (OPA/Conftest) and Terraform Cloud drift alerts may be helpful.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-8-0T8:00:00.000+0000\",\"updated\":\"2024-08-10T13:00:00.000+0000\",\"customfield_10016\":1,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-110/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"We’ve had several tickets from users in Hungary and Germany — reports show corrupted characters in downloaded PDFs.\",\"created\":\"2024-08-6T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Confirmed — accented characters render as `?` or boxes in the exported file. Looks like a font embedding issue.\",\"created\":\"2024-08-6T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Most likely the library defaults to a basic font without full UTF-8 coverage. We’ll need to embed an appropriate font manually.\",\"created\":\"2024-08-6T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Trying Noto Sans with UTF-8 encoding — initial tests show proper character rendering on multiple locales.\",\"created\":\"2024-08-6T13:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Will also add unit tests for PDF exports in multiple languages to prevent regression in the future.\",\"created\":\"2024-08-6T14:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"Thanks! Let’s prioritize a hotfix rollout once verified by QA — this is blocking several enterprise users.\",\"created\":\"2024-08-6T15:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-9T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-10T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-10T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"111\",\"key\":\"DEV-111\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/111\",\"fields\":{\"summary\":\"Sprint start dates are incorrectly parsed in time zones with DST\",\"description\":\"Sprints created through the Jira integration API have incorrect start and end dates when the board is configured in a timezone that observes Daylight Saving Time. For example, in Europe/Budapest, a sprint set to start at 09:00 on a Monday appears as 08:00 in our database. The issue stems from the backend incorrectly parsing the ISO 8601 timestamp without accounting for DST shifts. This causes mismatches between the times shown in Jira and those used in our analytics backend. As a result, sprint duration calculations and overlap detection logic are off by one hour in affected regions. This causes misalignment in planning tools, report generation, and notification scheduling. The problem affects all boards using DST-aware zones and causes cumulative data skew over multiple sprints. A thorough audit of our datetime parsing and conversion logic is required, especially around Z-suffix usage and OffsetDateTime conversions.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-7-11T8:00:00.000+0000\",\"updated\":\"2024-08-10T10:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-111/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"Have we confirmed whether the issue occurs only during the DST transition periods, or does it affect all sprint start dates year-round in affected time zones?\",\"created\":\"2024-08-8T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"The bug is consistent year-round for DST-aware time zones — the backend uses a fixed offset parser (Z suffix) instead of respecting local time zone rules. So even outside the transition dates, any time expressed as \\\"local time\\\" gets shifted incorrectly when interpreted as UTC.\",\"created\":\"2024-08-8T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Are we using OffsetDateTime or ZonedDateTime in our parsing logic? If it's the former, that would explain why DST is ignored completely.\",\"created\":\"2024-08-8T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"Exactly — we currently deserialize all incoming timestamps as OffsetDateTime, which locks in the numeric offset (e.g. +01:00) but discards timezone information like Europe/Budapest. We’ll need to migrate the parsing layer to use ZonedDateTime and retain the full region-based time zone so DST rules are correctly applied. A patch is in progress for the Jira integration module.\",\"created\":\"2024-08-8T13:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":5,\"total\":5,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-8T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40031\",\"created\":\"2024-08-9T11:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]},{\"id\":\"40032\",\"created\":\"2024-08-9T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10002\",\"fromString\":\"Blocked\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-10T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-10T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"112\",\"key\":\"DEV-112\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/112\",\"fields\":{\"summary\":\"Invalid JSON stored in text field of ticket embeddings\",\"description\":\"The text field of the jira_ticket_embeddings table, which should store valid JSON, occasionally contains malformed strings that break downstream processing. The corruption seems to happen intermittently when tickets are updated in rapid succession, possibly due to a race condition in the transformation logic. We've observed fields being truncated, nested structures being flattened incorrectly, and missing quotes around property names. This leads to deserialization failures during vector re-indexing and causes the related tickets to be excluded from semantic search. In worst cases, the entire sprint-level retrieval fails because of one corrupted entry. Our error logs do not catch these serialization issues early enough, and there is no schema validation at the DB level. We need to implement stricter input sanitation and introduce a test to ensure all stored text values conform to a defined JSON schema before insertion or update.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-7-21T8:00:00.000+0000\",\"updated\":\"2024-08-11T15:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-112/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"We’ve had several tickets from users in Hungary and Germany — reports show corrupted characters in downloaded PDFs.\",\"created\":\"2024-08-4T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Confirmed — accented characters render as `?` or boxes in the exported file. Looks like a font embedding issue.\",\"created\":\"2024-08-4T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Most likely the library defaults to a basic font without full UTF-8 coverage. We’ll need to embed an appropriate font manually.\",\"created\":\"2024-08-4T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Trying Noto Sans with UTF-8 encoding — initial tests show proper character rendering on multiple locales.\",\"created\":\"2024-08-4T13:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Will also add unit tests for PDF exports in multiple languages to prevent regression in the future.\",\"created\":\"2024-08-4T14:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"Thanks! Let’s prioritize a hotfix rollout once verified by QA — this is blocking several enterprise users.\",\"created\":\"2024-08-4T15:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-8T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-11T12:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-11T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"113\",\"key\":\"DEV-113\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/113\",\"fields\":{\"summary\":\"Sprint filtering ignores tickets without story points\",\"description\":\"The backend filtering logic for sprint-level metrics skips tickets that do not have assigned story points, assuming they are irrelevant. However, in many teams, unestimated tickets are deliberately included in sprints and should be tracked. Because these tickets are ignored, metrics like ticket count, throughput, and issue type distribution are skewed. This particularly affects reporting on sprint completion rates and affects teams practicing continuous estimation. The logic currently uses a WHERE story_points IS NOT NULL clause that should be removed or replaced with an optional flag. Users are unaware of the filtering, leading to incorrect assumptions in dashboards and planning tools. Furthermore, embedding-based features trained on full sprint data also end up missing a subset of tickets, introducing subtle bias. A code audit is needed to locate all the filters that exclude these tickets and ensure consistent behavior across all endpoints.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-3-28T8:00:00.000+0000\",\"updated\":\"2024-08-12T15:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-113/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-11T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-12T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-12T17:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"114\",\"key\":\"DEV-114\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/114\",\"fields\":{\"summary\":\"Ticket detail modal fails to render markdown formatting\",\"description\":\"The ticket detail modal does not correctly render Jira-style markdown formatting such as bullet points, numbered lists, bold or italic text. Instead, raw syntax like *bold*, - list item, or {code} is displayed literally in the UI. This occurs on both the ticket preview and full modal views, indicating that the markdown renderer is either missing or misconfigured. The bug was introduced after a recent refactor where the rendering component was swapped out for performance reasons. As a result, descriptions and comments look cluttered and unreadable, especially for longer tickets. Users can no longer rely on formatting to structure their content, leading to confusion and lower productivity. This also affects accessibility, since unstructured blobs of text are harder to parse visually. We need to reintroduce markdown parsing, possibly using a lightweight library that supports Jira-style syntax or extend an existing parser to include custom tokens.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-5-1T8:00:00.000+0000\",\"updated\":\"2024-08-11T15:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-114/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-9T9:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-11T10:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-11T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"115\",\"key\":\"DEV-115\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/115\",\"fields\":{\"summary\":\"Story point selection UI becomes unresponsive after reconnection\",\"description\":\"When a user reconnects to the planning session after a temporary network drop, the story point selection buttons remain visible but unresponsive. Clicking them produces no effect, and no WebSocket messages are sent to the backend. This seems to happen when the frontend's internal state is not reset properly after a WebSocket reconnection event. The Redux store (or equivalent state manager) still holds the stale connection state, causing the event handlers to be bound to a dead socket. Users are forced to refresh the entire browser tab to regain interactivity. This affects the user experience during critical planning moments and leads to inconsistent participation data. Logs show that reconnection happens at the transport level, but component-level lifecycle hooks are not retriggered. A full teardown and re-initialization of WebSocket bindings is likely missing. The reconnect event should reset all relevant UI state and rebroadcast any queued actions to avoid desync.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Medium\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-7-4T8:00:00.000+0000\",\"updated\":\"2024-08-12T17:00:00.000+0000\",\"customfield_10016\":3,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-115/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Does the issue reproduce reliably if we simulate a network drop using DevTools and then restore connection after a few seconds? Just want to confirm it's not race-condition dependent.\",\"created\":\"2024-08-10T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Yes, it’s 100% reproducible that way. The WebSocket reconnects at the transport layer, but the vote handler components don’t rebind. The button click still fires a Redux action, but the underlying socket reference is stale and silently fails.\",\"created\":\"2024-08-10T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Do we currently handle socket reinitialization in a global handler or per-component effect? It sounds like the local UI doesn't get rehydrated when the socket comes back.\",\"created\":\"2024-08-10T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Right now, reconnection logic lives in a global hook, but components don’t subscribe to socket lifecycle events. We’ll need to emit a custom \\\"reconnected\\\" event from the global layer, then trigger a full teardown and rebind in each story interaction component. Also planning to queue any in-flight vote actions and replay them after rebind.\",\"created\":\"2024-08-10T13:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-11T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40033\",\"created\":\"2024-08-12T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40034\",\"created\":\"2024-08-12T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"116\",\"key\":\"DEV-116\",\"self\":\"https://our-domain.atlassian.net/rest/api/2/issue/116\",\"fields\":{\"summary\":\"CI pipeline fails intermittently due to Docker layer cache corruption\",\"description\":\"Our main CI pipeline fails randomly on the Docker image build step with a no space left on device or failed to extract layer error. This seems to be caused by a corrupted or exhausted Docker layer cache on the GitHub Actions runners or self-hosted agents. Cleaning the cache manually resolves the issue temporarily, but it returns after a few runs. The problem is more frequent on builds that involve multiple stages and large base images, especially when switching between branches with different dependency trees. We currently rely on default caching behavior without explicitly managing space or purging old layers. This instability breaks the developer feedback loop and wastes significant CI credits. A more robust cache eviction strategy and regular cleanup job are required, along with limits on retained layers. We should also explore using remote Docker registries with cache mounts or prebuilt layers to reduce build variance.\",\"issuetype\":{\"name\":\"Story\"},\"priority\":{\"name\":\"Low\"},\"status\":{\"name\":\"In Progress\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"created\":\"2025-7-1T8:00:00.000+0000\",\"updated\":\"2024-08-12T14:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-116/watchers\",\"watchCount\":2},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"body\":\"Has anyone checked if the cache corruption only happens on self-hosted runners, or is it also affecting the default GitHub-hosted runners? I'm wondering if our disk space limits differ.\",\"created\":\"2024-08-10T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Good question — I ran a few test builds on both. It happens on both environments, but it’s more frequent on the self-hosted runners. Looks like they accumulate layers faster without automatic cleanup.\",\"created\":\"2024-08-10T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"body\":\"Would switching to using buildx with remote cache storage (like AWS S3 or GCR) help here? That way we can avoid depending on the runner’s local disk space.\",\"created\":\"2024-08-10T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Dave Leard\"},\"body\":\"Yes, using buildx with a remote cache is actually one of the recommended solutions for this. I can prototype that in a separate branch and compare build times and cache hit rates.\",\"created\":\"2024-08-10T12:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":1,\"total\":1,\"histories\":[{\"id\":\"40030\",\"created\":\"2024-08-10T13:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede217\",\"displayName\":\"James Turner\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"117\",\"key\":\"DEV-117\",\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/117\",\"fields\":{\"summary\":\"Session timer desync causes voting to close prematurely for some users\",\"description\":\"During planning poker sessions, users report that the voting interface closes unexpectedly while others are still selecting story points. The issue appears to stem from client-side session timers that drift out of sync with the server’s authoritative voting state. In some cases, clients start countdowns slightly earlier or later due to network latency or delayed WebSocket initialization, causing the vote submission window to close inconsistently across participants. As a result, users are either locked out before making a selection or their votes are silently rejected without warning. The server correctly enforces a central timeout, but the frontend displays a local timer that does not reflect this source of truth. Additionally, if the client tab is inactive or throttled by the browser, the timer may stop updating entirely. This bug undermines user trust in the fairness of the process and creates confusion during team discussions. A solution should involve syncing the remaining time periodically with the server, displaying live status updates over WebSocket, and disabling local countdown authority entirely. All vote-related actions should be gated behind the server’s current session phase, not inferred locally. The UI should also clearly indicate whether the vote window is open or closed based on server confirmation, not just elapsed time. A full review of the session timer logic and proper fallback handling is needed to prevent premature closure and allow consistent user experience across all participants.\",\"issuetype\":{\"name\":\"Bug\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Done\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-7-1T8:00:00.000+0000\",\"updated\":\"2024-08-8T12:00:00.000+0000\",\"customfield_10016\":5,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-117/watchers\",\"watchCount\":1},\"attachment\":[],\"comment\":[],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40001\",\"created\":\"2024-08-3T8:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40004\",\"created\":\"2024-08-5T15:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10003\",\"toString\":\"Under Review\"}]},{\"id\":\"40005\",\"created\":\"2024-08-5T18:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10003\",\"fromString\":\"Under Review\",\"to\":\"10004\",\"toString\":\"Done\"}]}]}}", "jiraSprintId": 1
}

###

POST {{host}}/api/v1/jira-issue
Content-Type: application/json
Accept: application/json
User-Agent: IntelliJ HTTP Client/IntelliJ IDEA 2024.3.5
Accept-Encoding: br, deflate, gzip, x-gzip

{
  "jiraIssueJson": "{\"id\":\"118\",\"key\":\"DEV-118\",\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/118\",\"fields\":{\"summary\":\"Server-side voting session lifecycle fails to propagate phase updates consistently\",\"description\":\"The backend logic managing the planning poker voting session does not consistently propagate phase transitions—such as moving from \\\"voting\\\" to \\\"closed\\\"—to all connected clients via WebSocket. This results in situations where some clients still think voting is open and attempt to submit story points, while others have already transitioned to the results view. Root cause analysis shows that the phase transition event is emitted only to a subset of WebSocket sessions, often depending on which nodes in the cluster the users are connected to. Our current WebSocket broadcasting implementation uses in-memory publish-subscribe mechanisms without cross-node awareness, which breaks in horizontally scaled environments. Additionally, there is no retry or confirmation logic ensuring that all clients received the transition update, and missed events cause desynchronization. Because of this, the backend may silently reject late votes without the client being aware, resulting in user frustration and lost inputs. A robust server-side session manager is needed to track the exact state of each session centrally and rebroadcast critical phase changes through a distributed messaging layer such as Redis Pub/Sub or Kafka. Clients should receive periodic sync messages containing the current canonical state, including voting status and remaining time, to recover from any missed messages. We also need to implement audit logging for state transitions and rejected votes to assist with debugging and QA verification. Until this is resolved, the voting experience will remain unreliable in multi-node deployments and during network instability scenarios.\",\"issuetype\":{\"name\":\"Bug\"},\"priority\":{\"name\":\"High\"},\"status\":{\"name\":\"Blocked\"},\"assignee\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"reporter\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"created\":\"2025-7-1T8:00:00.000+0000\",\"updated\":\"2024-08-12T12:00:00.000+0000\",\"customfield_10016\":8,\"watcher\":{\"isWatching\":true,\"self\":\"https://your-domain.atlassian.net/rest/api/2/issue/ED-118/watchers\",\"watchCount\":1},\"attachment\":[],\"comment\":[{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Do we know if this desync only happens when the cluster is scaled beyond a single node? I can’t seem to reproduce it locally with just one instance.\",\"created\":\"2024-08-1T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Correct — it's cluster-related. In production where we run 3 nodes, the WebSocket events don't propagate across instances because we're using in-memory pub-sub. Locally it looks fine since everything is on the same process.\",\"created\":\"2024-08-1T9:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Would introducing Redis Pub/Sub solve the cross-node event propagation? Or do we need something more robust like Kafka for ordering guarantees?\",\"created\":\"2024-08-2T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Redis Pub/Sub would already be a big step up and is lightweight enough for our use case. Kafka would give ordering guarantees but adds more ops overhead. I’d recommend starting with Redis.\",\"created\":\"2024-08-3T10:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"Also, should we add a periodic sync message from server to client with the canonical session state? That way if a client misses an event, it can recover on the next sync.\",\"created\":\"2024-08-5T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede216\",\"displayName\":\"Olivia Parker\"},\"body\":\"Yes please — right now clients fully trust the WebSocket events, and there's no fallback. A periodic session state broadcast would fix a lot of these ghost-state bugs.\",\"created\":\"2024-08-8T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede214\",\"displayName\":\"Sarah Bennett\"},\"body\":\"Agree. I’ll add a heartbeat that pushes out the current session phase + remaining time every few seconds. That will help clients resync if they miss the initial transition event.\",\"created\":\"2024-08-9T11:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede211\",\"displayName\":\"John Carter\"},\"body\":\"Can we also log rejected late votes server-side? QA has no way today to verify which votes got silently dropped during testing.\",\"created\":\"2024-08-9T12:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"body\":\"Good call — I can add audit logging for all rejected votes with session ID, user ID, and reason (e.g., late submission). That will help us debug desync reports.\",\"created\":\"2024-08-10T12:00:00.000+0000\"},{\"author\":{\"accountId\":\"5b10a2844c20165700ede212\",\"displayName\":\"Emily Watson\"},\"body\":\"Let’s make sure we prioritize this fix. The desyncs are causing users to lose trust in the voting tool, especially during large planning sessions. Once Redis + sync + audit logs are in, we can unblock this ticket.\",\"created\":\"2024-08-10T13:00:00.000+0000\"}],\"issuelinks\":[],\"subTasks\":[],\"worklog\":[],\"timetracking\":null},\"changelog\":{\"startAt\":0,\"maxResults\":3,\"total\":3,\"histories\":[{\"id\":\"40001\",\"created\":\"2024-08-10T8:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10000\",\"fromString\":\"To Do\",\"to\":\"10001\",\"toString\":\"In Progress\"}]},{\"id\":\"40002\",\"created\":\"2024-08-12T16:00:00.000+0000\",\"author\":{\"accountId\":\"5b10a2844c20165700ede213\",\"displayName\":\"Michael Brooks\"},\"items\":[{\"field\":\"status\",\"fieldtype\":\"jira\",\"from\":\"10001\",\"fromString\":\"In Progress\",\"to\":\"10002\",\"toString\":\"Blocked\"}]}]}}", "jiraSprintId": 1
}

###